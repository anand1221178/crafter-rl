# DrQ-v2 Implementation Plan - Anand's Work
**Data-Regularized Q-Learning v2 for Crafter Environment**

## Algorithm Overview
DrQ-v2 is a model-free RL algorithm specifically designed for visual control tasks. It combines DQN with data augmentation to achieve state-of-the-art sample efficiency on pixel-based environments.

**Key Innovations:**
- **Data Augmentation**: Random crops and color jittering during training
- **Improved Architecture**: Better CNN encoders for visual observations
- **Target Networks**: Soft updates for stability
- **Sample Efficiency**: Achieves strong performance with limited data

## Iterative Development Strategy

### ðŸ“Š BASELINE - EVALUATION 1
**Goal**: Establish baseline performance with barebones DQN

#### Implementation Scope:
```python
# Core Components (Phase 1)
DrQv2Agent(BaseAgent)           # Main agent inheriting from base
â”œâ”€â”€ QNetwork                    # CNN: 64x64 RGB â†’ Q-values (17 actions)
â”œâ”€â”€ ReplayBuffer               # Basic circular buffer (100k transitions)
â”œâ”€â”€ act()                      # Epsilon-greedy action selection
â”œâ”€â”€ store_experience()         # Store (s, a, r, s', done) tuples
â””â”€â”€ update()                   # Standard Q-learning update
```

#### Technical Details:
- **Network Architecture**: 3-layer CNN + 2-layer MLP
- **Learning Rate**: 3e-4
- **Batch Size**: 32
- **Epsilon**: 1.0 â†’ 0.01 over 100k steps
- **Target Update**: Hard update every 1000 steps
- **Replay Buffer**: 100k transitions, uniform sampling

#### Expected Performance:
- **Crafter Score**: 0.5-1.0% (basic DQN baseline)
- **Achievements**: collect_wood, place_table (easiest ones)
- **Episode Length**: ~50-100 steps
- **Training**: Slow initial learning, may struggle with visual complexity

---

### ðŸš€ IMPROVEMENT 1 - EVALUATION 2
**Goal**: Add core DrQ-v2 features (data augmentation + architecture)

#### New Components:
```python
# Data Augmentation Module
DataAugmentation
â”œâ”€â”€ RandomCrop(84 â†’ 64)         # Spatial augmentation
â”œâ”€â”€ ColorJitter                 # Brightness/contrast changes
â””â”€â”€ augment_batch()             # Apply during training only

# Improved Architecture
QNetworkV2                      # Better CNN encoder
â”œâ”€â”€ Larger receptive fields     # 3x3 â†’ 5x5 kernels
â”œâ”€â”€ More channels              # 32 â†’ 64 â†’ 128
â””â”€â”€ Batch normalization        # Stabilize training
```

#### Key Changes:
1. **Data Augmentation Pipeline**:
   - Random crops from 84x84 to 64x64 (Crafter's native size)
   - Color jittering: brightness Â±0.2, contrast Â±0.2
   - Apply only during training (not evaluation)

2. **Improved Q-Network**:
   - Better CNN encoder with more capacity
   - Batch normalization for stability
   - Proper weight initialization

3. **Target Network Improvements**:
   - Soft updates (Ï„ = 0.01) instead of hard updates
   - More stable Q-value estimates

#### Expected Performance:
- **Crafter Score**: 2.0-4.0% (+100-300% improvement)
- **New Achievements**: make_wood_pickaxe, defeat_skeleton, collect_stone
- **Episode Length**: ~100-200 steps
- **Sample Efficiency**: Faster learning due to augmentation

---

### âš¡ IMPROVEMENT 2 - EVALUATION 3
**Goal**: Advanced optimizations for maximum performance

#### Advanced Components:
```python
# Prioritized Experience Replay
PrioritizedReplayBuffer
â”œâ”€â”€ TD-error based sampling     # Focus on important transitions
â”œâ”€â”€ Importance sampling weights # Correct bias from non-uniform sampling
â””â”€â”€ Priority decay             # Reduce importance over time

# Multi-step Learning
NStepReturns
â”œâ”€â”€ n=3 step returns           # Look ahead 3 steps
â”œâ”€â”€ Discounted reward sum      # R_t + Î³R_{t+1} + Î³Â²R_{t+2} + Î³Â³V_{t+3}
â””â”€â”€ More stable targets        # Reduce variance

# Double Q-Learning
DoubleQUpdates                  # Reduce overestimation bias
â”œâ”€â”€ Action selection network    # Choose action with main network
â””â”€â”€ Value evaluation network    # Evaluate with target network
```

#### Technical Enhancements:
1. **Prioritized Replay**:
   - Sample transitions based on TD-error magnitude
   - Higher probability for surprising/important experiences
   - Importance sampling weights to correct bias

2. **N-step Returns (n=3)**:
   - Multi-step temporal difference learning
   - More stable targets, faster propagation of rewards
   - Balance between bias (n=1) and variance (n=âˆž)

3. **Double Q-Learning**:
   - Decouple action selection from value evaluation
   - Reduces overestimation bias common in Q-learning
   - More accurate Q-value estimates

4. **Training Optimizations**:
   - Gradient clipping (norm=10.0)
   - Learning rate scheduling
   - Improved exploration schedule

#### Expected Performance:
- **Crafter Score**: 4.0-8.0% (competitive with published results)
- **Advanced Achievements**: collect_iron, make_iron_tools, defeat_zombie
- **Episode Length**: ~200-500 steps
- **Robustness**: More consistent performance across runs

---

## Implementation Timeline

### Week 1: Baseline Implementation
- [ ] Create `src/agents/drqv2_agent.py` with BaseAgent inheritance
- [ ] Implement basic Q-network architecture
- [ ] Create simple replay buffer
- [ ] Integrate with `train.py`
- [ ] **Run Evaluation 1**: Baseline performance

### Week 2: Core DrQ-v2 Features
- [ ] Implement data augmentation pipeline
- [ ] Improve Q-network architecture
- [ ] Add soft target updates
- [ ] Debug and optimize training
- [ ] **Run Evaluation 2**: Measure improvement from augmentation

### Week 3: Advanced Optimizations
- [ ] Implement prioritized experience replay
- [ ] Add n-step returns computation
- [ ] Integrate double Q-learning
- [ ] Fine-tune hyperparameters
- [ ] **Run Evaluation 3**: Final performance

### Week 4: Analysis & Comparison
- [ ] Compare all three evaluations
- [ ] Analyze which components contributed most
- [ ] Generate plots and reports
- [ ] Prepare algorithm explanation for report

---

## File Structure

```
src/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ base_agent.py           # Shared interface (TODO: complete for DrQ-v2)
â”‚   â””â”€â”€ drqv2_agent.py         # Main DrQ-v2 implementation
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ replay_buffer.py       # Basic + Prioritized replay buffers
â”‚   â”œâ”€â”€ data_augmentation.py   # Random crops, color jittering
â”‚   â””â”€â”€ networks.py            # Q-network architectures
â””â”€â”€ evaluation/
    â””â”€â”€ (existing evaluation tools)
```

## Evaluation Strategy

After each phase:
```bash
# Quick development check (10 episodes)
python test_model.py models/drqv2_v{1,2,3}.zip drqv2 10

# Full evaluation for report (100 episodes)
python evaluate.py --model_path models/drqv2_v{1,2,3}.zip --algorithm drqv2 --episodes 100
```

### Key Metrics to Track:
1. **Crafter Score** (geometric mean of achievements) - Main benchmark
2. **Individual Achievement Rates** - Which ones improve?
3. **Sample Efficiency** - Performance vs. training steps
4. **Episode Length** - Survival time indicator
5. **Training Stability** - Consistent improvement?

## Success Criteria

### Minimum Viable Performance:
- **Baseline**: Score â‰¥ 0.5%, basic achievements unlocked
- **Improvement 1**: Score â‰¥ 2.0%, clear benefit from augmentation
- **Improvement 2**: Score â‰¥ 4.0%, competitive with baseline algorithms

### Stretch Goals:
- **Score > 6.0%**: Competitive with DreamerV2 (6.8%)
- **Advanced Achievements**: collect_iron, make_iron_sword, defeat_zombie
- **Sample Efficiency**: Good performance within 1M steps

## Common Issues & Solutions

### Training Problems:
- **Slow Convergence**: Increase learning rate, reduce batch size
- **Unstable Training**: Add gradient clipping, reduce target update frequency
- **Poor Exploration**: Tune epsilon schedule, add noise to actions
- **Memory Issues**: Reduce replay buffer size, use smaller networks

### Environment-Specific:
- **Sparse Rewards**: Focus on survival reward initially
- **Visual Complexity**: Start with simpler augmentations
- **Long Episodes**: Use proper discounting (Î³=0.99)
- **Achievement Unlock**: Monitor specific achievement progress

## Research Context

**Original DrQ-v2 Paper**: [Data-Efficient Reinforcement Learning with Self-Predictive Representations](https://arxiv.org/abs/2107.09645)

**Key Insights for Report**:
1. Data augmentation is crucial for visual RL sample efficiency
2. Random crops work better than other augmentations for control
3. Soft target updates provide more stable learning
4. Architecture improvements compound with augmentation benefits

**Crafter-Specific Adaptations**:
- 64x64 RGB observations (vs. typical 84x84)
- 17 discrete actions (vs. continuous control)
- Sparse achievement rewards + dense survival rewards
- Long episodes with hierarchical objectives

---
*Last Updated: 2025-09-29*
*Owner: Anand Patel*